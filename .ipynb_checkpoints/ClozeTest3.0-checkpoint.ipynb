{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T06:52:02.289013Z",
     "start_time": "2019-12-20T06:52:02.261920Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob \n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import collections\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from array import array\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import treebank\n",
    "from textblob import TextBlob\n",
    "from nltk import Tree\n",
    "from nltk.chunk.regexp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:32:25.788330Z",
     "start_time": "2019-12-19T02:32:25.781731Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitSentence(paragraph):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(paragraph)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:53:59.595230Z",
     "start_time": "2019-12-19T01:53:59.590964Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NONE_STR = '<s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:54:00.001922Z",
     "start_time": "2019-12-19T01:53:59.990024Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_other_letter(line):\n",
    "    punctuation =\"\"\"!\"#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~\\\\n\"\"\"\n",
    "    re_punctuation =\"[{}]+\".format(punctuation)\n",
    "    line =re.sub(re_punctuation, \"\", line)\n",
    "    return line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:54:16.005924Z",
     "start_time": "2019-12-19T01:54:15.983995Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_articles(paths):\n",
    "    alllines = []\n",
    "    for path in paths:\n",
    "        filenames = glob.glob(path+\"/*txt\")\n",
    "        for filename in filenames:\n",
    "            with open(filename, 'r', encoding='utf-8') as fpr:\n",
    "                data_raw = json.load(fpr)\n",
    "                article = data_raw['article']\n",
    "                \n",
    "#                 lines = article.split('.')\n",
    "                lines = splitSentence(article)\n",
    "                for line in lines:\n",
    "                    line = delete_other_letter(line)\n",
    "                    alllines.append(line)\n",
    "                \n",
    "    return alllines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:33:04.201995Z",
     "start_time": "2019-12-19T02:32:32.649422Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = read_articles(['./RACE/train/high','./RACE/train/middle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:33:04.208303Z",
     "start_time": "2019-12-19T02:33:04.204493Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443717\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:55:07.496795Z",
     "start_time": "2019-12-19T01:55:07.450568Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_dict(lines,n):\n",
    "                \n",
    "    pre_words = collections.defaultdict(int)\n",
    "    be_words = collections.defaultdict(int)\n",
    "    n = n-1\n",
    "    for line in lines:\n",
    "        for k in range(n):\n",
    "            line = NONE_STR+' '+line+' '+NONE_STR\n",
    "        line_words = line.split()\n",
    "        for i in range(len(line_words)-(n-1)):\n",
    "            ngramTemp = ' '.join(line_words[i:i+n])\n",
    "\n",
    "            pre_words[ngramTemp] += 1\n",
    "            be_words[line_words[i]] += 1\n",
    "            \n",
    "    return pre_words,be_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:55:08.002721Z",
     "start_time": "2019-12-19T01:55:07.994811Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_punctuation(word):\n",
    "    for puc in punctuation_list:\n",
    "        word = word.replace(puc,'')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:16:55.070500Z",
     "start_time": "2019-12-19T02:16:55.064002Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_Chinese_punctuation(content):\n",
    "    content = content.replace('’','\\'')\n",
    "    content = content.replace('”','\"')\n",
    "    content = content.replace('“','\"')\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:55:08.398514Z",
     "start_time": "2019-12-19T01:55:08.373675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_options(options_str):\n",
    "    options_str = replace_Chinese_punctuation(options_str)\n",
    "    options = []\n",
    "    opt_rows = options_str.split('#')\n",
    "    for opt in opt_rows:\n",
    "        indexA = opt.find('A.')\n",
    "        indexB = opt.find('B.')\n",
    "        indexC = opt.find('C.')\n",
    "        indexD = opt.find('D.')\n",
    "        As = opt[indexA+2:indexB].strip().lower().split(' ')\n",
    "        Bs = opt[indexB+2:indexC].strip().lower().split(' ')\n",
    "        Cs = opt[indexC+2:indexD].strip().lower().split(' ')\n",
    "        Ds = opt[indexD+2:].strip().lower().split(' ')\n",
    "        options.append([As,Bs,Cs,Ds])\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:30:50.206414Z",
     "start_time": "2019-12-21T04:30:50.169322Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_data(ngram=3):\n",
    "    path = './data'\n",
    "    filenames = glob.glob(path+\"/*txt\")\n",
    "    \n",
    "    answers_list =[]\n",
    "    options_list = []\n",
    "    content_list = [] #问题空的前后缀\n",
    "    sentence_list = [] #问题所在句子\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r', encoding='utf-8') as fpr:\n",
    "            data_raw = json.load(fpr)\n",
    "            article = data_raw['article']\n",
    "            content = get_pre_and_be_words(article,ngram)\n",
    "            sentence = get_question_sentences(article)\n",
    "            options_str = data_raw['options']\n",
    "            options = get_options(options_str)\n",
    "            answers = list(data_raw['answers'])\n",
    "            answers_list.append(answers)\n",
    "            options_list.append(options)\n",
    "            content_list.append(content)\n",
    "            sentence_list.append(sentence)\n",
    "    return content_list,sentence_list,options_list,answers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:40:34.079539Z",
     "start_time": "2019-12-21T04:40:34.065731Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_options_are_the_same_tag(options):\n",
    "    tag = ''\n",
    "    for opt in options:\n",
    "        sen_pos = nltk.pos_tag(nltk.word_tokenize(opt[0]))\n",
    "        opt_tag = sen_pos[0][1]\n",
    "        if tag == '':\n",
    "            tag = opt_tag\n",
    "        elif tag != opt_tag:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:40:36.079425Z",
     "start_time": "2019-12-21T04:40:36.024902Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pre_and_be_words(article,ngram):\n",
    "    article = replace_Chinese_punctuation(article)\n",
    "    art_words = article.split()\n",
    "    n = len(art_words)\n",
    "    index = 1\n",
    "    content = []\n",
    "    for i in range(n):\n",
    "        word = art_words[i]\n",
    "        if str(index) == word:\n",
    "            pre_word_list = []\n",
    "            be_word_list = []\n",
    "            if i < ngram-1:\n",
    "                for k in range(ngram-1-i):\n",
    "                    pre_word_list.append(NONE_STR)\n",
    "            else:\n",
    "                pre_word_list = art_words[i-1-(ngram-2):i]\n",
    "            pre_word_list = check_punctuation_and_replace_with_none(pre_word_list,True)\n",
    "\n",
    "            if i > n-ngram:\n",
    "                for k in range(ngram-1-(n-ngram)):\n",
    "                    be_word_list.append(NONE_STR)\n",
    "            else:\n",
    "                be_word_list = art_words[i+1:i+1+ngram-1]\n",
    "            be_word_list = check_punctuation_and_replace_with_none(be_word_list,False)\n",
    "            \n",
    "            content.append([pre_word_list,be_word_list])\n",
    "            index += 1\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动做出选择的代码，逻辑如下\n",
    "\n",
    "\n",
    "1.先判断4个选项的词性，相同则转2，不同则转3\n",
    "\n",
    "2.如果能找到有效分块则采用模型2.0\n",
    "\n",
    "3.如果不能则采用trigram模型2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:54:36.140767Z",
     "start_time": "2019-12-21T04:54:36.113608Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probability_of_block(block,option,opt_pos):\n",
    "        \n",
    "    if block is None:\n",
    "        return 0\n",
    "    \n",
    "    n = len(option)\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            block[opt_pos]=option[i]\n",
    "            continue\n",
    "        block.insert(opt_pos+i,option[i])\n",
    "    \n",
    "    ngram = len(block)\n",
    "    if ngram > 4:\n",
    "        return 0\n",
    "    \n",
    "    pre_word = ' '.join(block[0:ngram-1])\n",
    "    prob = math.log(probability(pre_word,block[ngram-1],ngram))\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:54:11.903493Z",
     "start_time": "2019-12-21T04:54:11.854816Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_select_model(question,options,ngram=3,block=None,opt_pos=-1,rate=0):\n",
    "    index = 0\n",
    "    maxprob = -10000000000\n",
    "    n = len(options)\n",
    "    for i in range(n):\n",
    "        \n",
    "        option = options[i]\n",
    "        block_prob = get_probability_of_block(block,option,opt_pos)\n",
    "        \n",
    "        word_list = []\n",
    "        word_list = question[0].copy()\n",
    "        word_list.append(options[i][0])\n",
    "        word_list.extend(question[1])\n",
    "        wln = len(word_list)\n",
    "        prob = 0\n",
    "        for k in range(wln-ngram+1):\n",
    "            pre_word = ' '.join(word_list[k:k+ngram-1])\n",
    "            prob += math.log(probability(pre_word,word_list[k+ngram-1],ngram))\n",
    "        \n",
    "        if block_prob > 0:\n",
    "            prob = block_prob*rate+prob*(1-rate)\n",
    "        if prob > maxprob:\n",
    "            maxprob = prob\n",
    "            index = i\n",
    "    return chr(ord('A')+index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T05:03:05.421257Z",
     "start_time": "2019-12-21T05:03:05.390097Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_select(question,sentence,options,index):\n",
    "    #对于模型2.0 ngram默认用3\n",
    "    ngram = 3\n",
    "    flag = is_options_are_the_same_tag(options)    \n",
    "    if flag:\n",
    "        print('--',sentence)\n",
    "        print('---',index)\n",
    "        print('----',options)\n",
    "        block,opt_pos = get_sentence_option_block(sentence,index,options)\n",
    "        print(block,opt_pos)\n",
    "        if block != None:\n",
    "            ngram = len(block)\n",
    "            print('使用model3.0做出的预测')\n",
    "            print('有用的分块',block)\n",
    "            answer = auto_select_model(question,options,ngram=ngram,block=block,opt_pos=opt_pos)\n",
    "            return answer\n",
    "    print('使用model2.0做出的预测')\n",
    "    answer = auto_select_model(question,options,ngram)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T05:15:43.597920Z",
     "start_time": "2019-12-21T05:15:43.481018Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------1---------\n",
      "before: [['since', \"i'd\"], ['seen', 'him']]\n",
      "使用model2.0做出的预测\n",
      "after: [['since', \"i'd\"], ['seen', 'him']]\n",
      "predict: A ground truth: D\n",
      "空前后的词 [['since', \"i'd\"], ['seen', 'him']]\n",
      "填空所在的句子 He'd moved to England with his mum when he was three and it had been 13 years since I'd  1  seen him.\n",
      "选项 [['also'], ['often'], ['even'], ['last']]\n",
      "---------2---------\n",
      "before: [['imagine', 'my'], ['when', 'he']]\n",
      "-- So imagine my  2   when he emailed me saying he wanted to come to visit me.\n",
      "--- 2\n",
      "---- [['delight'], ['relief'], ['anger'], ['worry']]\n",
      "##\n",
      "['So', 'imagine', 'my', 'delight'] 3\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['So', 'imagine', 'my', 'delight']\n",
      "after: [['imagine', 'my'], ['when', 'he']]\n",
      "predict: A ground truth: A\n",
      "空前后的词 [['imagine', 'my'], ['when', 'he']]\n",
      "填空所在的句子 So imagine my  2   when he emailed me saying he wanted to come to visit me.\n",
      "选项 [['delight'], ['relief'], ['anger'], ['worry']]\n",
      "---------3---------\n",
      "before: [['i', 'was'], ['<s>', '<s>']]\n",
      "-- I was   3  !\n",
      "--- 3\n",
      "---- [['scared'], ['shocked'], ['thrilled'], ['ashamed']]\n",
      "##\n",
      "['was', 'scared'] 1\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['was', 'scared']\n",
      "after: [['i', 'was'], ['<s>', '<s>']]\n",
      "predict: B ground truth: C\n",
      "空前后的词 [['i', 'was'], ['<s>', '<s>']]\n",
      "填空所在的句子 I was   3  !\n",
      "选项 [['scared'], ['shocked'], ['thrilled'], ['ashamed']]\n",
      "---------4---------\n",
      "before: [['supposed', 'to'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [['supposed', 'to'], ['<s>', '<s>']]\n",
      "predict: C ground truth: C\n",
      "空前后的词 [['supposed', 'to'], ['<s>', '<s>']]\n",
      "填空所在的句子 I arrived early at Byron Bay where we were supposed to   4  .\n",
      "选项 [['talk'], ['stay'], ['meet'], ['settle']]\n",
      "---------5---------\n",
      "before: [['bay', 'was'], ['in', 'sunshine']]\n",
      "使用model2.0做出的预测\n",
      "after: [['bay', 'was'], ['in', 'sunshine']]\n",
      "predict: A ground truth: A\n",
      "空前后的词 [['bay', 'was'], ['in', 'sunshine']]\n",
      "填空所在的句子 The bay was  5   in sunshine, and there was a group of kayakers around 150m off the shore.\n",
      "选项 [['bathed'], ['clean'], ['deep'], ['formed']]\n",
      "---------6---------\n",
      "before: [['a', 'little'], ['<s>', '<s>']]\n",
      "-- Getting a little   6  , I realized one kayak  was in  7  .\n",
      "--- 6\n",
      "---- [['faster'], ['closer'], ['heavier'], ['wiser']]\n",
      "##\n",
      "['Getting', 'a', 'little', 'faster'] 3\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['Getting', 'a', 'little', 'faster']\n",
      "after: [['a', 'little'], ['<s>', '<s>']]\n",
      "predict: C ground truth: B\n",
      "空前后的词 [['a', 'little'], ['<s>', '<s>']]\n",
      "填空所在的句子 Getting a little   6  , I realized one kayak  was in  7  .\n",
      "选项 [['faster'], ['closer'], ['heavier'], ['wiser']]\n",
      "---------7---------\n",
      "before: [['was', 'in'], ['<s>', '<s>']]\n",
      "-- Getting a little   6  , I realized one kayak  was in  7  .\n",
      "--- 7\n",
      "---- [['trouble'], ['advance'], ['question'], ['battle']]\n",
      "##\n",
      "['was', 'in', 'trouble'] 2\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['was', 'in', 'trouble']\n",
      "after: [['was', 'in'], ['<s>', '<s>']]\n",
      "predict: A ground truth: A\n",
      "空前后的词 [['was', 'in'], ['<s>', '<s>']]\n",
      "填空所在的句子 Getting a little   6  , I realized one kayak  was in  7  .\n",
      "选项 [['trouble'], ['advance'], ['question'], ['battle']]\n",
      "---------8---------\n",
      "before: [[\"something's\", 'not'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [[\"something's\", 'not'], ['<s>', '<s>']]\n",
      "predict: C ground truth: B\n",
      "空前后的词 [[\"something's\", 'not'], ['<s>', '<s>']]\n",
      "填空所在的句子 \"Something's not   8  !\"\n",
      "选项 [['real'], ['right'], ['fair'], ['fit']]\n",
      "---------9---------\n",
      "before: [['t-shirt', 'and'], ['into', 'the']]\n",
      "使用model2.0做出的预测\n",
      "after: [['t-shirt', 'and'], ['into', 'the']]\n",
      "predict: C ground truth: C\n",
      "空前后的词 [['t-shirt', 'and'], ['into', 'the']]\n",
      "填空所在的句子 I took off my T-shirt and  9   into the water.\n",
      "选项 [['stared'], ['sank'], ['dived'], ['fell']]\n",
      "---------10---------\n",
      "before: [['he', 'was'], ['violently', '<s>']]\n",
      "-- He was  10   violently.\n",
      "--- 10\n",
      "---- [['arguing'], ['fighting'], ['shouting'], ['shaking']]\n",
      "##\n",
      "['was', 'arguing', 'violently'] 1\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['was', 'arguing', 'violently']\n",
      "after: [['he', 'was'], ['violently', '<s>']]\n",
      "predict: D ground truth: D\n",
      "空前后的词 [['he', 'was'], ['violently', '<s>']]\n",
      "填空所在的句子 He was  10   violently.\n",
      "选项 [['arguing'], ['fighting'], ['shouting'], ['shaking']]\n",
      "---------11---------\n",
      "before: [['i', 'helped'], ['the', 'young']]\n",
      "使用model2.0做出的预测\n",
      "after: [['i', 'helped'], ['the', 'young']]\n",
      "predict: D ground truth: C\n",
      "空前后的词 [['i', 'helped'], ['the', 'young']]\n",
      "填空所在的句子 Linking arms with one of the instructors， I helped  11   the young man out of the water.\n",
      "选项 [['lead'], ['persuade'], ['carry'], ['keep']]\n",
      "---------12---------\n",
      "before: [['<s>', 'something'], ['to', 'me']]\n",
      "使用model2.0做出的预测\n",
      "after: [['<s>', 'something'], ['to', 'me']]\n",
      "predict: B ground truth: B\n",
      "空前后的词 [['<s>', 'something'], ['to', 'me']]\n",
      "填空所在的句子 He was unconscious and as I looked at his face, something  12   to me.\n",
      "选项 [['happened'], ['occurred'], ['applied'], ['appealed']]\n",
      "---------13---------\n",
      "before: [['were', 'very'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [['were', 'very'], ['<s>', '<s>']]\n",
      "predict: D ground truth: D\n",
      "空前后的词 [['were', 'very'], ['<s>', '<s>']]\n",
      "填空所在的句子 Those brown eyes were very   13  .\n",
      "选项 [['sharp'], ['pleasant'], ['attractive'], ['familiar']]\n",
      "---------14---------\n",
      "before: [['immediately', 'i'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [['immediately', 'i'], ['<s>', '<s>']]\n",
      "predict: B ground truth: D\n",
      "空前后的词 [['immediately', 'i'], ['<s>', '<s>']]\n",
      "填空所在的句子 \"Ben,\" he replied, and immediately I  14  .\n",
      "选项 [['agreed'], ['hesitated'], ['doubted'], ['knew']]\n",
      "---------15---------\n",
      "before: [['<s>', '<s>'], ['<s>', '<s>']]\n",
      "-- 15 , after a brief stay in hospital, Ben was well enough to be allowed to  16   and later the family met up for dinner.\n",
      "--- 15\n",
      "---- [['fortunately'], ['frankly'], ['sadly'], ['suddenly']]\n",
      "##\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-386-194571db77e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswers_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshowAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-375-07553afe6bf7>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(content_list, sentence_list, options_list, answers_list, showAll, predict)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-381-6620a37b8123>\u001b[0m in \u001b[0;36mauto_select\u001b[0;34m(question, sentence, options, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_option_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "test(content_list,sentence_list,options_list,answers_list,showAll=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:55:37.713807Z",
     "start_time": "2019-12-21T04:55:37.624629Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(content_list,sentence_list,options_list,answers_list,showAll=False,predict=None):\n",
    "    \n",
    "    cn = len(content_list)\n",
    "    right_n = 0\n",
    "    total_n = 0\n",
    "    for k in range(cn):\n",
    "        content = content_list[k]\n",
    "        answers = answers_list[k]\n",
    "        options = options_list[k]\n",
    "        sentence = sentence_list[k]\n",
    "    \n",
    "        n = len(answers)\n",
    "        total_n += n\n",
    "        for i in range(n):\n",
    "            if showAll:\n",
    "                print('---------'+str(i+1)+'---------')\n",
    "            if predict is None:\n",
    "                print('before:',content[i])\n",
    "                choice = auto_select(content[i],sentence[i],options[i],i+1)\n",
    "                print('after:',content[i])\n",
    "            else:\n",
    "                choice = predict\n",
    "            if choice == answers[i]:\n",
    "                right_n += 1\n",
    "                if showAll == False:\n",
    "                    print('---------'+str(i+1)+'---------')\n",
    "                    print('predict:',choice,'ground truth:',answers[i])\n",
    "                    print('空前后的词',content[i])\n",
    "                    print('填空所在的句子',sentence[i])\n",
    "                    print('选项',options[i])\n",
    "            if showAll:\n",
    "                print('predict:',choice,'ground truth:',answers[i])\n",
    "                print('空前后的词',content[i])\n",
    "                print('填空所在的句子',sentence[i])\n",
    "                print('选项',options[i])\n",
    "    accuracy = float(right_n)/total_n\n",
    "    print('accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:15.307077Z",
     "start_time": "2019-12-21T04:05:15.258844Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_matrix(pre_words,be_words,lines,n):  \n",
    "    n = n-1\n",
    "    for line in tqdm(lines):\n",
    "        for k in range(n):\n",
    "            line = NONE_STR+' '+line+' '+NONE_STR\n",
    "        line_words = line.split()\n",
    "        words_n = len(line_words)-n\n",
    "        for k in range(words_n):\n",
    "            pre_word = ' '.join(line_words[k:k+n])\n",
    "                \n",
    "            be_word = line_words[k+n]\n",
    "            \n",
    "            if pre_word in pre_words and be_word in be_words:\n",
    "                i = pre_words.index(pre_word)\n",
    "                j = be_words.index(be_word)\n",
    "                words_matrix[i][j] += 1\n",
    "                    \n",
    "#             if n > 1:\n",
    "#                 if pre_word in pre_words:\n",
    "#                     i = pre_words.index(pre_word)\n",
    "#                     j = be_words.index(be_word)\n",
    "#                     words_matrix[i][j] += 1\n",
    "#             else:\n",
    "#                 i = pre_words.index(pre_word)\n",
    "#                 j = be_words.index(be_word)\n",
    "#                 words_matrix[i][j] += 1\n",
    "            \n",
    "    return words_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:55:31.102743Z",
     "start_time": "2019-12-19T01:55:31.064857Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_word_num(lines,n):\n",
    "    pre_words = collections.defaultdict(int)\n",
    "    be_words = collections.defaultdict(int)\n",
    "    \n",
    "    words = collections.defaultdict(int)\n",
    "    n = n-1\n",
    "    \n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        for k in range(n):\n",
    "            line = NONE_STR+' '+line+' '+NONE_STR\n",
    "        line_words = line.split()\n",
    "        for i in range(len(line_words)-(n-1)):\n",
    "            pre_word = ' '.join(line_words[i-1:i+n-1])\n",
    "            be_word = line_words[i+n-1]\n",
    "            \n",
    "            pre_words[pre_word] += 1\n",
    "            be_words[be_word] += 1\n",
    "            \n",
    "            words[pre_word+' '+be_word] += 1\n",
    "    return pre_words,be_words,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:55:31.503058Z",
     "start_time": "2019-12-19T01:55:31.497871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_list = ['.',',','?','!','\\\"','“']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T01:55:33.207347Z",
     "start_time": "2019-12-19T01:55:33.119370Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_punctuation_and_replace_with_none(word_list,reverse = False):\n",
    "    n = len(word_list)\n",
    "    f_flag = False #第一个字符是否为标点\n",
    "    b_flag = False #最后一个字符是否为标点\n",
    "    if reverse == True:\n",
    "        for i in range(n-1, -1, -1):\n",
    "            word = word_list[i]\n",
    "            if b_flag == False:\n",
    "                for puc in punctuation_list:\n",
    "                    if word[-1]==puc :\n",
    "                        b_flag = True \n",
    "                        break\n",
    "                    elif word[0] == puc:\n",
    "                        f_flag = True\n",
    "                        break\n",
    "            if b_flag == True:\n",
    "                word_list[i] = NONE_STR\n",
    "            else:\n",
    "                word_list[i] = replace_punctuation(word).lower()\n",
    "            if f_flag == True:\n",
    "                b_flag = True\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            word = word_list[i]\n",
    "            if f_flag == False:\n",
    "                for puc in punctuation_list:\n",
    "                    if word[0] == puc:\n",
    "                        f_flag = True\n",
    "                        break\n",
    "                    elif word[-1]==puc :\n",
    "                        b_flag = True \n",
    "                        break\n",
    "            if f_flag == True:\n",
    "                word_list[i] = NONE_STR\n",
    "            else:\n",
    "                word_list[i] = replace_punctuation(word).lower()\n",
    "            if b_flag == True:\n",
    "                f_flag = True\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T06:53:38.016977Z",
     "start_time": "2019-12-20T06:53:37.996665Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_question_sentences(article):\n",
    "    article = replace_Chinese_punctuation(article)\n",
    "    sentences = splitSentence(article)\n",
    "    \n",
    "    content = []\n",
    "    index = 1\n",
    "    for sentence in sentences:\n",
    "        art_words = sentence.split()\n",
    "        n = len(art_words)\n",
    "        for i in range(n):\n",
    "            word = art_words[i]\n",
    "            if str(index) == word:\n",
    "                content.append(sentence)\n",
    "                index += 1\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:54:43.479298Z",
     "start_time": "2019-12-19T02:54:43.470365Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_sparse_words(words_dict,threshold=4):\n",
    "    words_list = []\n",
    "    for key, value in words_dict.items():\n",
    "        if value > 4:\n",
    "            words_list.append(key)\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T03:11:40.728227Z",
     "start_time": "2019-12-21T03:11:40.711247Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(word1,word2,ngram):\n",
    "    pre_words = pre_words_list[ngram-2]\n",
    "    words = words_list[ngram-2]\n",
    "    \n",
    "    eclipse = 0.0000000000001\n",
    "    all = pre_words[word1]\n",
    "    single = words[word1+' '+word2]\n",
    "    \n",
    "    if all == 0:\n",
    "        return eclipse\n",
    "    \n",
    "    prob = float(single)/all\n",
    "    if prob == 0:\n",
    "        prob = eclipse\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=8 face=雅黑>使用NLTK将句子划分开</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算bi、tri、four-gram的数据，准备之后用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T02:51:29.361919Z",
     "start_time": "2019-12-21T02:50:19.913539Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_words_list = []\n",
    "be_words_list = []\n",
    "words_list = []\n",
    "for i in range(2,5):\n",
    "    pre_words,be_words,words = count_word_num(lines,i)\n",
    "    pre_words_list.append(pre_words)\n",
    "    be_words_list.append(be_words)\n",
    "    words_list.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:34:14.035078Z",
     "start_time": "2019-12-21T04:34:14.020173Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "content_list,sentence_list,options_list,answers_list = read_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:34:14.864608Z",
     "start_time": "2019-12-21T04:34:14.855827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t-shirt', 'and'], ['into', 'the']]\n",
      "I took off my T-shirt and  9   into the water.\n"
     ]
    }
   ],
   "source": [
    "print(content_list[0][8])\n",
    "print(sentence_list[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:23.922966Z",
     "start_time": "2019-12-21T04:05:23.917832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stared'], ['sank'], ['dived'], ['fell']]\n"
     ]
    }
   ],
   "source": [
    "print(options_list[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:25.125667Z",
     "start_time": "2019-12-21T04:05:25.090420Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_block(sentence):\n",
    "    test_sent_pos = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    sentence_tree = reg_parser.parse(test_sent_pos)\n",
    "    # sentence_tree.draw()\n",
    "\n",
    "    def get_words_from_tree(tree,word_list):\n",
    "        if isinstance(tree, Tree): \n",
    "            for i in range(len(tree)):\n",
    "                get_words_from_tree(tree[i],word_list)\n",
    "        else:\n",
    "            word_list.append(tree[0])\n",
    "\n",
    "\n",
    "    temp_list = []\n",
    "    for child in sentence_tree:\n",
    "        if isinstance(child, Tree):               \n",
    "            if child.label() == 'VERB_CON' or child.label() == 'NP':\n",
    "                if len(child)>1 and len(child)<5:\n",
    "                    item = []\n",
    "                    get_words_from_tree(child,item)\n",
    "                    temp_list.append(item)\n",
    "\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:25.589916Z",
     "start_time": "2019-12-21T04:05:25.563207Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def does_list_exits_in_other_list(list1,list2):\n",
    "    index = -1\n",
    "    pos = -1\n",
    "    n = len(list1)\n",
    "    flag = False\n",
    "    for opt in list2:\n",
    "        for i in range(index+1,n):\n",
    "            word = list1[i]\n",
    "            if flag == True:\n",
    "                if opt != word:\n",
    "                    return False\n",
    "                else:\n",
    "                    index += 1\n",
    "                    break\n",
    "            else:\n",
    "                if opt == word:\n",
    "                    index = i\n",
    "                    pos = index\n",
    "                    flag = True\n",
    "                    break\n",
    "    return flag,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T05:18:28.112279Z",
     "start_time": "2019-12-21T05:18:28.090048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_option_block(sentence,index,options):\n",
    "    #用A选项替换空中的标号\n",
    "    A = options[0]\n",
    "    sentence = sentence.replace(' '+str(index)+' ',' '.join(A))\n",
    "    sentence = sentence.replace(str(index)+' ',' '.join(A))\n",
    "    sentence = sentence.replace(' '+str(index),' '.join(A))\n",
    "    #获得句子分块\n",
    "    blocks = get_sentence_block(sentence)\n",
    "    #找到分块中与选项相关的部分\n",
    "    for block in blocks:\n",
    "        flag,pos = does_list_exits_in_other_list(block,A)\n",
    "        if flag:\n",
    "            return block,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:27.121042Z",
     "start_time": "2019-12-21T04:05:27.116665Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"He'd moved to England with his mum when he was three and it had been 13 years since I'd  1  seen him.\", 'So imagine my  2   when he emailed me saying he wanted to come to visit me.', 'I was   3  !', 'I arrived early at Byron Bay where we were supposed to   4  .', 'The bay was  5   in sunshine, and there was a group of kayakers around 150m off the shore.', 'Getting a little   6  , I realized one kayak  was in  7  .', 'Getting a little   6  , I realized one kayak  was in  7  .', '\"Something\\'s not   8  !\"', 'I took off my T-shirt and  9   into the water.', 'He was  10   violently.', 'Linking arms with one of the instructors， I helped  11   the young man out of the water.', 'He was unconscious and as I looked at his face, something  12   to me.', 'Those brown eyes were very   13  .', '\"Ben,\" he replied, and immediately I  14  .', '15 , after a brief stay in hospital, Ben was well enough to be allowed to  16   and later the family met up for dinner.', '15 , after a brief stay in hospital, Ben was well enough to be allowed to  16   and later the family met up for dinner.', 'We chatted about everything and then Ben  17   to me.', '\"You  18   my life!\"', \"I still can't believe what a  19   it was.\", \"I'm just so glad I was there  20   to help my son.\"]\n"
     ]
    }
   ],
   "source": [
    "print(content_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:30.240831Z",
     "start_time": "2019-12-21T04:05:30.236461Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['also'], ['often'], ['even'], ['last']], [['delight'], ['relief'], ['anger'], ['worry']], [['scared'], ['shocked'], ['thrilled'], ['ashamed']], [['talk'], ['stay'], ['meet'], ['settle']], [['bathed'], ['clean'], ['deep'], ['formed']], [['faster'], ['closer'], ['heavier'], ['wiser']], [['trouble'], ['advance'], ['question'], ['battle']], [['real'], ['right'], ['fair'], ['fit']], [['stared'], ['sank'], ['dived'], ['fell']], [['arguing'], ['fighting'], ['shouting'], ['shaking']], [['lead'], ['persuade'], ['carry'], ['keep']], [['happened'], ['occurred'], ['applied'], ['appealed']], [['sharp'], ['pleasant'], ['attractive'], ['familiar']], [['agreed'], ['hesitated'], ['doubted'], ['knew']], [['fortunately'], ['frankly'], ['sadly'], ['suddenly']], [['return'], ['relax'], ['speak'], ['leave']], [['joked'], ['turned'], ['listened'], ['pointed']], [['created'], ['honored'], ['saved'], ['guided']], [['coincidence'], ['change'], ['pity'], ['pain']], [['on', 'board'], ['in', 'time'], ['for', 'sure'], ['on', 'purpose']]]\n"
     ]
    }
   ],
   "source": [
    "print(options_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T04:05:30.513097Z",
     "start_time": "2019-12-21T04:05:30.508376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'A', 'C', 'C', 'A', 'B', 'A', 'B', 'C', 'D', 'C', 'B', 'D', 'D', 'A', 'D', 'B', 'C', 'A', 'B']\n"
     ]
    }
   ],
   "source": [
    "print(answers_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T05:18:31.660059Z",
     "start_time": "2019-12-21T05:18:31.526687Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------1---------\n",
      "before: [['since', \"i'd\"], ['seen', 'him']]\n",
      "使用model2.0做出的预测\n",
      "after: [['since', \"i'd\"], ['seen', 'him']]\n",
      "predict: A ground truth: D\n",
      "空前后的词 [['since', \"i'd\"], ['seen', 'him']]\n",
      "填空所在的句子 He'd moved to England with his mum when he was three and it had been 13 years since I'd  1  seen him.\n",
      "选项 [['also'], ['often'], ['even'], ['last']]\n",
      "---------2---------\n",
      "before: [['imagine', 'my'], ['when', 'he']]\n",
      "-- So imagine my  2   when he emailed me saying he wanted to come to visit me.\n",
      "--- 2\n",
      "---- [['delight'], ['relief'], ['anger'], ['worry']]\n",
      "[['So', 'imagine', 'my', 'delight'], ['emailed', 'me'], ['saying', 'he'], ['wanted', 'to', 'come', 'to', 'visit', 'me']]\n",
      "['So', 'imagine', 'my', 'delight'] 3\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['So', 'imagine', 'my', 'delight']\n",
      "after: [['imagine', 'my'], ['when', 'he']]\n",
      "predict: A ground truth: A\n",
      "空前后的词 [['imagine', 'my'], ['when', 'he']]\n",
      "填空所在的句子 So imagine my  2   when he emailed me saying he wanted to come to visit me.\n",
      "选项 [['delight'], ['relief'], ['anger'], ['worry']]\n",
      "---------3---------\n",
      "before: [['i', 'was'], ['<s>', '<s>']]\n",
      "-- I was   3  !\n",
      "--- 3\n",
      "---- [['scared'], ['shocked'], ['thrilled'], ['ashamed']]\n",
      "[['was', 'scared']]\n",
      "['was', 'scared'] 1\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['was', 'scared']\n",
      "after: [['i', 'was'], ['<s>', '<s>']]\n",
      "predict: B ground truth: C\n",
      "空前后的词 [['i', 'was'], ['<s>', '<s>']]\n",
      "填空所在的句子 I was   3  !\n",
      "选项 [['scared'], ['shocked'], ['thrilled'], ['ashamed']]\n",
      "---------4---------\n",
      "before: [['supposed', 'to'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [['supposed', 'to'], ['<s>', '<s>']]\n",
      "predict: C ground truth: C\n",
      "空前后的词 [['supposed', 'to'], ['<s>', '<s>']]\n",
      "填空所在的句子 I arrived early at Byron Bay where we were supposed to   4  .\n",
      "选项 [['talk'], ['stay'], ['meet'], ['settle']]\n",
      "---------5---------\n",
      "before: [['bay', 'was'], ['in', 'sunshine']]\n",
      "使用model2.0做出的预测\n",
      "after: [['bay', 'was'], ['in', 'sunshine']]\n",
      "predict: A ground truth: A\n",
      "空前后的词 [['bay', 'was'], ['in', 'sunshine']]\n",
      "填空所在的句子 The bay was  5   in sunshine, and there was a group of kayakers around 150m off the shore.\n",
      "选项 [['bathed'], ['clean'], ['deep'], ['formed']]\n",
      "---------6---------\n",
      "before: [['a', 'little'], ['<s>', '<s>']]\n",
      "-- Getting a little   6  , I realized one kayak  was in  7  .\n",
      "--- 6\n",
      "---- [['faster'], ['closer'], ['heavier'], ['wiser']]\n",
      "[['Getting', 'a', 'little', 'faster'], ['was', 'in']]\n",
      "['Getting', 'a', 'little', 'faster'] 3\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['Getting', 'a', 'little', 'faster']\n",
      "after: [['a', 'little'], ['<s>', '<s>']]\n",
      "predict: C ground truth: B\n",
      "空前后的词 [['a', 'little'], ['<s>', '<s>']]\n",
      "填空所在的句子 Getting a little   6  , I realized one kayak  was in  7  .\n",
      "选项 [['faster'], ['closer'], ['heavier'], ['wiser']]\n",
      "---------7---------\n",
      "before: [['was', 'in'], ['<s>', '<s>']]\n",
      "-- Getting a little   6  , I realized one kayak  was in  7  .\n",
      "--- 7\n",
      "---- [['trouble'], ['advance'], ['question'], ['battle']]\n",
      "[['Getting', 'a', 'little'], ['was', 'in', 'trouble']]\n",
      "['was', 'in', 'trouble'] 2\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['was', 'in', 'trouble']\n",
      "after: [['was', 'in'], ['<s>', '<s>']]\n",
      "predict: A ground truth: A\n",
      "空前后的词 [['was', 'in'], ['<s>', '<s>']]\n",
      "填空所在的句子 Getting a little   6  , I realized one kayak  was in  7  .\n",
      "选项 [['trouble'], ['advance'], ['question'], ['battle']]\n",
      "---------8---------\n",
      "before: [[\"something's\", 'not'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [[\"something's\", 'not'], ['<s>', '<s>']]\n",
      "predict: C ground truth: B\n",
      "空前后的词 [[\"something's\", 'not'], ['<s>', '<s>']]\n",
      "填空所在的句子 \"Something's not   8  !\"\n",
      "选项 [['real'], ['right'], ['fair'], ['fit']]\n",
      "---------9---------\n",
      "before: [['t-shirt', 'and'], ['into', 'the']]\n",
      "使用model2.0做出的预测\n",
      "after: [['t-shirt', 'and'], ['into', 'the']]\n",
      "predict: C ground truth: C\n",
      "空前后的词 [['t-shirt', 'and'], ['into', 'the']]\n",
      "填空所在的句子 I took off my T-shirt and  9   into the water.\n",
      "选项 [['stared'], ['sank'], ['dived'], ['fell']]\n",
      "---------10---------\n",
      "before: [['he', 'was'], ['violently', '<s>']]\n",
      "-- He was  10   violently.\n",
      "--- 10\n",
      "---- [['arguing'], ['fighting'], ['shouting'], ['shaking']]\n",
      "[['was', 'arguing', 'violently']]\n",
      "['was', 'arguing', 'violently'] 1\n",
      "使用model3.0做出的预测\n",
      "有用的分块 ['was', 'arguing', 'violently']\n",
      "after: [['he', 'was'], ['violently', '<s>']]\n",
      "predict: D ground truth: D\n",
      "空前后的词 [['he', 'was'], ['violently', '<s>']]\n",
      "填空所在的句子 He was  10   violently.\n",
      "选项 [['arguing'], ['fighting'], ['shouting'], ['shaking']]\n",
      "---------11---------\n",
      "before: [['i', 'helped'], ['the', 'young']]\n",
      "使用model2.0做出的预测\n",
      "after: [['i', 'helped'], ['the', 'young']]\n",
      "predict: D ground truth: C\n",
      "空前后的词 [['i', 'helped'], ['the', 'young']]\n",
      "填空所在的句子 Linking arms with one of the instructors， I helped  11   the young man out of the water.\n",
      "选项 [['lead'], ['persuade'], ['carry'], ['keep']]\n",
      "---------12---------\n",
      "before: [['<s>', 'something'], ['to', 'me']]\n",
      "使用model2.0做出的预测\n",
      "after: [['<s>', 'something'], ['to', 'me']]\n",
      "predict: B ground truth: B\n",
      "空前后的词 [['<s>', 'something'], ['to', 'me']]\n",
      "填空所在的句子 He was unconscious and as I looked at his face, something  12   to me.\n",
      "选项 [['happened'], ['occurred'], ['applied'], ['appealed']]\n",
      "---------13---------\n",
      "before: [['were', 'very'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [['were', 'very'], ['<s>', '<s>']]\n",
      "predict: D ground truth: D\n",
      "空前后的词 [['were', 'very'], ['<s>', '<s>']]\n",
      "填空所在的句子 Those brown eyes were very   13  .\n",
      "选项 [['sharp'], ['pleasant'], ['attractive'], ['familiar']]\n",
      "---------14---------\n",
      "before: [['immediately', 'i'], ['<s>', '<s>']]\n",
      "使用model2.0做出的预测\n",
      "after: [['immediately', 'i'], ['<s>', '<s>']]\n",
      "predict: B ground truth: D\n",
      "空前后的词 [['immediately', 'i'], ['<s>', '<s>']]\n",
      "填空所在的句子 \"Ben,\" he replied, and immediately I  14  .\n",
      "选项 [['agreed'], ['hesitated'], ['doubted'], ['knew']]\n",
      "---------15---------\n",
      "before: [['<s>', '<s>'], ['<s>', '<s>']]\n",
      "-- 15 , after a brief stay in hospital, Ben was well enough to be allowed to  16   and later the family met up for dinner.\n",
      "--- 15\n",
      "---- [['fortunately'], ['frankly'], ['sadly'], ['suddenly']]\n",
      "[['a', 'brief', 'stay'], ['was', 'well', 'enough', 'to', 'be', 'allowed', 'to'], ['later', 'the', 'family'], ['met', 'up', 'for', 'dinner']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-397-194571db77e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswers_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshowAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-375-07553afe6bf7>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(content_list, sentence_list, options_list, answers_list, showAll, predict)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-381-6620a37b8123>\u001b[0m in \u001b[0;36mauto_select\u001b[0;34m(question, sentence, options, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_option_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "test(content_list,sentence_list,options_list,answers_list,showAll=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "功能测试代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T01:54:03.803075Z",
     "start_time": "2019-12-21T01:54:03.750347Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I/PRP)\n",
      "  (VERB_CON (VERB took/VBD off/RP) (NP my/PRP$ T-shirt/NN))\n",
      "  and/CC\n",
      "  (VERB_CON (VERB stared/VBD) (P into/IN) (NP the/DT water/NN))\n",
      "  ./.)\n",
      "[['took', 'off', 'my', 'T-shirt'], ['stared', 'into', 'the', 'water']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'I took off my T-shirt and  stared    into the water.'\n",
    "# opt_word = \n",
    "\n",
    "# ‘<XX>’ 代表XX类型的词 ‘<XX>?’代表必须含有这个类型的词1或者0次 '<XX>*‘表示含有有这个类型的词0或者多次 '<XX>+'表示一定含有这个词\n",
    "\n",
    "# NP DT:限定词 JJ:形容词 NNP:专有名词 NN:名词单数 NNS:名词复数\n",
    "# Preposition\n",
    "# Verb\n",
    "# PP -> P NP\n",
    "# VP -> V (NP|PP)*\n",
    "\n",
    "reg_parser = RegexpParser('''\n",
    "        NP: {<PRP.*>?<MD>?<PRP$>?<DT>?<JJ.*>*<NN.*>*}   \n",
    "         P: {<IN>}    \n",
    "      VERB: {<RB.*>?<V.*>?<RB.*>?<TO>*<RP>?}                \n",
    "  VERB_CON: {<VERB>+<NP>?<P>?<NP>?}             \n",
    "''')\n",
    "\n",
    "test_sent = \"Mr. Obama played a big role in the insurance bill\"\n",
    "# test_sent = \"He'd moved to England with his mum\"\n",
    "test_sent_pos = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "sentence_tree = reg_parser.parse(test_sent_pos)\n",
    "print(sentence_tree)\n",
    "# sentence_tree.draw()\n",
    "\n",
    "def get_words_from_tree(tree,word_list):\n",
    "    if isinstance(tree, Tree): \n",
    "        for i in range(len(tree)):\n",
    "            get_words_from_tree(tree[i],word_list)\n",
    "    else:\n",
    "        word_list.append(tree[0])\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "for child in sentence_tree:\n",
    "    if isinstance(child, Tree):               \n",
    "        if child.label() == 'VERB_CON' or child.label() == 'NP':\n",
    "            if len(child)>1 and len(child)<5:\n",
    "                item = []\n",
    "                get_words_from_tree(child,item)\n",
    "                temp_list.append(item)\n",
    "                \n",
    "print(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T02:42:10.568894Z",
     "start_time": "2019-12-21T02:42:10.561091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stared', 'into', 'the', 'water']\n"
     ]
    }
   ],
   "source": [
    "block = get_sentence_option_block(content_list[0][8],9,options_list[0][8])\n",
    "print(block)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
